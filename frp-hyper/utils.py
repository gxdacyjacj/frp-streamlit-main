# -*- coding: utf-8 -*-
"""
FRP é’¢ç­‹è€ä¹…æ€§é¢„æµ‹ - å·¥å…·å‡½æ•°æ¨¡å—
Utility Functions for FRP Rebar Durability Prediction

åŒ…å«ï¼š
- sklearnå…¼å®¹æ€§è¡¥ä¸
- å®‰å…¨æ¨¡å‹åŠ è½½
- é€šç”¨å·¥å…·å‡½æ•°
"""

import base64
import pickle
import numpy as np
import pandas as pd
from sklearn.preprocessing import OneHotEncoder, StandardScaler, PolynomialFeatures
from sklearn.feature_selection import SelectKBest, f_regression
from sklearn.pipeline import Pipeline
from sklearn.compose import ColumnTransformer
from sklearn.preprocessing import FunctionTransformer
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score
import warnings

def apply_sklearn_compatibility_patch():
    """Apply patches for sklearn compatibility issues"""
    try:
        from sklearn.compose import _column_transformer
        
        # Patch missing _RemainderColsList if needed
        if not hasattr(_column_transformer, '_RemainderColsList'):
            class _RemainderColsList(list):
                """Compatibility class for older sklearn versions"""
                def __init__(self, remainder_columns):
                    super().__init__(remainder_columns)
                    self.remainder_columns = remainder_columns
            
            _column_transformer._RemainderColsList = _RemainderColsList
            print("Applied _RemainderColsList compatibility patch")
        
        return True
    except Exception as e:
        print(f"Failed to apply compatibility patch: {e}")
        return False

def safe_pickle_load(pickled_data):
    """Safely load pickled model data with version compatibility handling"""
    try:
        # First try normal loading
        return pickle.loads(pickled_data)
    except AttributeError as e:
        # Handle sklearn version compatibility issues
        if "_RemainderColsList" in str(e):
            apply_sklearn_compatibility_patch()
            try:
                return pickle.loads(pickled_data)
            except Exception as retry_e:
                print(f"Retry after patch failed: {retry_e}")
                return None
        else:
            print(f"Pickle loading failed: {e}")
            return None
    except Exception as e:
        print(f"General pickle loading error: {e}")
        return None

def load_model_from_base64(base64_data):
    """Load model from base64 encoded pickle data with error handling"""
    try:
        pickled_data = base64.b64decode(base64_data)
        return safe_pickle_load(pickled_data)
    except Exception as e:
        print(f"Base64 model loading failed: {e}")
        return None

def global_clean_categorical_features(X):
    """Global function to clean categorical features - can be pickled"""
    X_clean = X.copy()
    for col in X_clean.columns:
        X_clean[col] = X_clean[col].fillna('unknown').astype(str)
    return X_clean

def create_enhanced_preprocessor(categorical_cols, numeric_cols, add_polynomial=True, polynomial_degree=2):
    """Create enhanced preprocessor with optional polynomial features"""
    
    # Enhanced categorical transformer
    categorical_transformer = Pipeline([
        ('cleaner', FunctionTransformer(global_clean_categorical_features, validate=False)),
        ('encoder', OneHotEncoder(sparse_output=False, handle_unknown="ignore"))
    ])
    
    # Enhanced numeric transformer with optional polynomial features and feature selection
    if add_polynomial and len(numeric_cols) > 1:  # Only add if multiple numeric features
        numeric_transformer = Pipeline([
            ('scaler', StandardScaler()),
            ('poly', PolynomialFeatures(degree=polynomial_degree, interaction_only=True, include_bias=False)),
            ('selector', SelectKBest(f_regression, k=min(50, len(numeric_cols)*2)))  # Limit features
        ])
    else:
        # Use feature selection even without polynomial features
        if len(numeric_cols) > 10:  # Only if many features
            numeric_transformer = Pipeline([
                ('scaler', StandardScaler()),
                ('selector', SelectKBest(f_regression, k=min(20, len(numeric_cols))))  # Select best features
            ])
        else:
            numeric_transformer = StandardScaler()
    
    # Create preprocessor
    preprocessor = ColumnTransformer(
        transformers=[
            ('num', numeric_transformer, numeric_cols),
            ('cat', categorical_transformer, categorical_cols)
        ] if categorical_cols else [
            ('num', numeric_transformer, numeric_cols)
        ]
    )
    
    return preprocessor

def diagnose_model_performance(y_true, y_pred, model_name="Model"):
    """Diagnose model performance and return insights"""
    
    r2 = r2_score(y_true, y_pred)
    mse = mean_squared_error(y_true, y_pred)
    mae = mean_absolute_error(y_true, y_pred)
    rmse = np.sqrt(mse)
    
    # Calculate residuals
    residuals = y_true - y_pred
    
    # Check for patterns in residuals
    residual_std = np.std(residuals)
    residual_mean = np.mean(residuals)
    
    diagnosis = {
        'r2': r2,
        'mse': mse,
        'mae': mae,
        'rmse': rmse,
        'residual_bias': residual_mean,
        'residual_std': residual_std,
        'model_name': model_name
    }
    
    return diagnosis

def print_model_performance(diagnosis):
    """æ‰“å°æ¨¡å‹æ€§èƒ½è¯Šæ–­ç»“æœ"""
    print(f"\nğŸ“Š {diagnosis['model_name']} Performance Report:")
    print("=" * 50)
    print(f"RÂ² Score:      {diagnosis['r2']:.4f}")
    print(f"RMSE:          {diagnosis['rmse']:.4f}")
    print(f"MAE:           {diagnosis['mae']:.4f}")
    print(f"MSE:           {diagnosis['mse']:.4f}")
    print(f"Residual Bias: {diagnosis['residual_bias']:.4f}")
    print(f"Residual Std:  {diagnosis['residual_std']:.4f}")
    print("=" * 50)

def validate_dataframe(df, required_columns=None, name="DataFrame"):
    """éªŒè¯DataFrameçš„å®Œæ•´æ€§"""
    if df is None:
        raise ValueError(f"{name} is None")
    
    if df.empty:
        raise ValueError(f"{name} is empty")
    
    if required_columns:
        missing_cols = set(required_columns) - set(df.columns)
        if missing_cols:
            raise ValueError(f"{name} missing required columns: {missing_cols}")
    
    print(f"âœ… {name} validation passed: {df.shape}")
    return True

def safe_convert_to_numeric(series, default_value=0):
    """å®‰å…¨åœ°å°†seriesè½¬æ¢ä¸ºæ•°å€¼ç±»å‹"""
    try:
        # å°è¯•ç›´æ¥è½¬æ¢
        numeric_series = pd.to_numeric(series, errors='coerce')
        
        # å¡«å……NaNå€¼
        numeric_series = numeric_series.fillna(default_value)
        
        return numeric_series
    except Exception as e:
        print(f"Warning: Failed to convert to numeric: {e}")
        return pd.Series([default_value] * len(series), index=series.index)

def clean_column_names(df):
    """æ¸…ç†DataFrameçš„åˆ—å"""
    # ç§»é™¤å‰åç©ºæ ¼
    df.columns = df.columns.str.strip()
    
    # æ›¿æ¢ç‰¹æ®Šå­—ç¬¦
    df.columns = df.columns.str.replace(' ', '_')
    df.columns = df.columns.str.replace('[^a-zA-Z0-9_]', '', regex=True)
    
    return df

def save_model_safely(model, filepath, additional_info=None):
    """å®‰å…¨åœ°ä¿å­˜æ¨¡å‹"""
    import pickle
    import os
    
    try:
        # ç¡®ä¿ç›®å½•å­˜åœ¨
        os.makedirs(os.path.dirname(filepath), exist_ok=True)
        
        # ä¿å­˜æ¨¡å‹
        save_data = {
            'model': model,
            'additional_info': additional_info,
            'timestamp': pd.Timestamp.now()
        }
        
        with open(filepath, 'wb') as f:
            pickle.dump(save_data, f)
        
        print(f"âœ… Model saved successfully: {filepath}")
        return True
        
    except Exception as e:
        print(f"âŒ Failed to save model: {e}")
        return False

def load_model_safely(filepath):
    """å®‰å…¨åœ°åŠ è½½æ¨¡å‹"""
    import pickle
    
    try:
        with open(filepath, 'rb') as f:
            save_data = pickle.load(f)
        
        if isinstance(save_data, dict) and 'model' in save_data:
            print(f"âœ… Model loaded successfully: {filepath}")
            return save_data['model'], save_data.get('additional_info')
        else:
            # å‘åå…¼å®¹ï¼šç›´æ¥æ˜¯æ¨¡å‹å¯¹è±¡
            print(f"âœ… Legacy model loaded: {filepath}")
            return save_data, None
            
    except Exception as e:
        print(f"âŒ Failed to load model: {e}")
        return None, None


def train_validation_test_split(X, y, test_size=0.1, val_size=0.2, random_state=42):
    """
    å°†æ•°æ®æŒ‰ç…§7:2:1çš„æ¯”ä¾‹åˆ†å‰²ä¸ºè®­ç»ƒé›†ã€éªŒè¯é›†ã€æµ‹è¯•é›†
    
    Args:
        X: ç‰¹å¾æ•°æ®
        y: ç›®æ ‡å˜é‡
        test_size: æµ‹è¯•é›†æ¯”ä¾‹ (é»˜è®¤0.1, å³10%)
        val_size: éªŒè¯é›†æ¯”ä¾‹ (ä»å‰©ä½™æ•°æ®ä¸­çš„æ¯”ä¾‹, é»˜è®¤0.2, å³22.2%çš„æ€»æ•°æ®)
        random_state: éšæœºç§å­
        
    Returns:
        X_train, X_val, X_test, y_train, y_val, y_test
    """
    from sklearn.model_selection import train_test_split
    
    # ç¬¬ä¸€æ¬¡åˆ†å‰²: åˆ†å‡ºæµ‹è¯•é›†
    X_temp, X_test, y_temp, y_test = train_test_split(
        X, y, test_size=test_size, random_state=random_state
    )
    
    # ç¬¬äºŒæ¬¡åˆ†å‰²: ä»å‰©ä½™æ•°æ®ä¸­åˆ†å‡ºéªŒè¯é›†
    # val_sizeéœ€è¦è°ƒæ•´ä¸ºåœ¨å‰©ä½™æ•°æ®ä¸­çš„æ¯”ä¾‹
    val_size_adjusted = val_size / (1 - test_size)
    
    X_train, X_val, y_train, y_val = train_test_split(
        X_temp, y_temp, test_size=val_size_adjusted, random_state=random_state
    )
    
    print(f"ğŸ“Š æ•°æ®åˆ†å‰²ç»“æœ (7:2:1):")
    print(f"   è®­ç»ƒé›†: {X_train.shape[0]} æ ·æœ¬ ({X_train.shape[0]/len(X)*100:.1f}%)")
    print(f"   éªŒè¯é›†: {X_val.shape[0]} æ ·æœ¬ ({X_val.shape[0]/len(X)*100:.1f}%)")
    print(f"   æµ‹è¯•é›†: {X_test.shape[0]} æ ·æœ¬ ({X_test.shape[0]/len(X)*100:.1f}%)")
    print(f"   æ€»è®¡: {len(X)} æ ·æœ¬")
    
    return X_train, X_val, X_test, y_train, y_val, y_test


# åœ¨æ¨¡å—å¯¼å…¥æ—¶åº”ç”¨å…¼å®¹æ€§è¡¥ä¸
apply_sklearn_compatibility_patch()
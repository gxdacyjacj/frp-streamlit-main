#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
FRPè¶…å‚æ•°ä¼˜åŒ–å¿«é€Ÿæ¼”ç¤º
Quick Hyperparameter Optimization Demo for FRP Models

è¿™æ˜¯ä¸€ä¸ªç®€åŒ–çš„è¶…å‚æ•°ä¼˜åŒ–æ¼”ç¤ºè„šæœ¬ï¼Œç”¨äºå¿«é€Ÿæµ‹è¯•å’ŒéªŒè¯è¶…å‚æ•°ä¼˜åŒ–åŠŸèƒ½ã€‚
ç‚¹å‡»è¿è¡ŒæŒ‰é’®(â–¶ï¸)å³å¯æ‰§è¡Œï¼
"""

import numpy as np
import pandas as pd
from sklearn.ensemble import RandomForestRegressor
from sklearn.model_selection import train_test_split, GridSearchCV, RandomizedSearchCV
from sklearn.metrics import r2_score, mean_squared_error
from datetime import datetime
import time

def create_demo_frp_data(n_samples=500):
    """åˆ›å»ºFRPæ¼”ç¤ºæ•°æ®"""
    print("ğŸ¯ åˆ›å»ºFRPæ¼”ç¤ºæ•°æ®...")
    
    np.random.seed(42)
    
    # åˆ›å»ºç‰¹å¾æ•°æ®
    data = {
        'pH': np.random.uniform(5, 12, n_samples),
        'exposure_time': np.random.uniform(100, 2000, n_samples),
        'temperature': np.random.uniform(20, 80, n_samples),
        'fiber_content': np.random.uniform(0.1, 5.0, n_samples),
        'diameter': np.random.uniform(6, 16, n_samples),
        'load': np.random.uniform(0, 100, n_samples),
        'concrete': np.random.choice([0, 1], n_samples),
        'chloride': np.random.choice([0, 1], n_samples),
    }
    
    df = pd.DataFrame(data)
    
    # æ¨¡æ‹ŸçœŸå®çš„å¼ºåº¦ä¿ç•™ç‡å…³ç³»
    ph_effect = (df['pH'] - 7) ** 2 * -0.02
    temp_effect = df['temperature'] * -0.003
    time_effect = np.log(df['exposure_time']) * -0.08
    fiber_effect = df['fiber_content'] * 0.05
    
    retention = (0.85 + ph_effect + temp_effect + time_effect + fiber_effect + 
                np.random.normal(0, 0.08, n_samples))
    
    # é™åˆ¶åœ¨åˆç†èŒƒå›´
    df['tensile_retention'] = np.clip(retention, 0.3, 1.0)
    
    print(f"âœ… æ•°æ®åˆ›å»ºå®Œæˆ: {df.shape}")
    print(f"   ç›®æ ‡å˜é‡èŒƒå›´: {df['tensile_retention'].min():.3f} - {df['tensile_retention'].max():.3f}")
    
    return df

def run_hyperparameter_optimization():
    """è¿è¡Œè¶…å‚æ•°ä¼˜åŒ–å®éªŒ"""
    
    print("=" * 60)
    print("ğŸš€ FRPé’¢ç­‹è€ä¹…æ€§é¢„æµ‹ - è¶…å‚æ•°ä¼˜åŒ–æ¼”ç¤º")
    print("=" * 60)
    
    # 1. å‡†å¤‡æ•°æ®
    df = create_demo_frp_data(500)
    
    # 2. åˆ†ç¦»ç‰¹å¾å’Œç›®æ ‡
    X = df.drop('tensile_retention', axis=1)
    y = df['tensile_retention']
    
    # 3. æ•°æ®åˆ†å‰² (7:2:1)
    X_temp, X_test, y_temp, y_test = train_test_split(X, y, test_size=0.1, random_state=42)
    X_train, X_val, y_train, y_val = train_test_split(X_temp, y_temp, test_size=0.22, random_state=42)  # 0.22 * 0.9 â‰ˆ 0.2
    
    print(f"\nğŸ“Š æ•°æ®åˆ†å‰²:")
    print(f"   è®­ç»ƒé›†: {X_train.shape[0]} æ ·æœ¬ ({X_train.shape[0]/len(X)*100:.1f}%)")
    print(f"   éªŒè¯é›†: {X_val.shape[0]} æ ·æœ¬ ({X_val.shape[0]/len(X)*100:.1f}%)")
    print(f"   æµ‹è¯•é›†: {X_test.shape[0]} æ ·æœ¬ ({X_test.shape[0]/len(X)*100:.1f}%)")
    
    # 4. å®šä¹‰è¶…å‚æ•°æœç´¢ç©ºé—´
    param_grid = {
        'n_estimators': [50, 100, 200],
        'max_depth': [5, 10, 15, None],
        'min_samples_split': [2, 5, 10],
        'min_samples_leaf': [1, 2, 4]
    }
    
    param_random = {
        'n_estimators': [10, 50, 100, 200, 300],
        'max_depth': [3, 5, 10, 15, 20, None],
        'min_samples_split': [2, 5, 10, 20],
        'min_samples_leaf': [1, 2, 4, 8],
        'max_features': ['sqrt', 'log2', None]
    }
    
    results = []
    
    # 5. ç½‘æ ¼æœç´¢
    print(f"\nğŸ” å¼€å§‹ç½‘æ ¼æœç´¢è¶…å‚æ•°ä¼˜åŒ–...")
    start_time = time.time()
    
    rf_grid = RandomForestRegressor(random_state=42)
    grid_search = GridSearchCV(
        rf_grid, 
        param_grid, 
        cv=3, 
        scoring='r2', 
        n_jobs=-1, 
        verbose=1
    )
    
    grid_search.fit(X_train, y_train)
    
    # åœ¨éªŒè¯é›†ä¸Šè¯„ä¼°
    grid_pred = grid_search.predict(X_val)
    grid_r2 = r2_score(y_val, grid_pred)
    grid_rmse = np.sqrt(mean_squared_error(y_val, grid_pred))
    grid_time = time.time() - start_time
    
    results.append({
        'method': 'ç½‘æ ¼æœç´¢',
        'best_params': grid_search.best_params_,
        'val_r2': grid_r2,
        'val_rmse': grid_rmse,
        'time': grid_time
    })
    
    print(f"âœ… ç½‘æ ¼æœç´¢å®Œæˆ!")
    print(f"   æœ€ä½³å‚æ•°: {grid_search.best_params_}")
    print(f"   éªŒè¯é›†RÂ²: {grid_r2:.4f}")
    print(f"   éªŒè¯é›†RMSE: {grid_rmse:.4f}")
    print(f"   è€—æ—¶: {grid_time:.1f}ç§’")
    
    # 6. éšæœºæœç´¢
    print(f"\nğŸ² å¼€å§‹éšæœºæœç´¢è¶…å‚æ•°ä¼˜åŒ–...")
    start_time = time.time()
    
    rf_random = RandomForestRegressor(random_state=42)
    random_search = RandomizedSearchCV(
        rf_random, 
        param_random, 
        n_iter=30,
        cv=3, 
        scoring='r2', 
        n_jobs=-1, 
        random_state=42,
        verbose=1
    )
    
    random_search.fit(X_train, y_train)
    
    # åœ¨éªŒè¯é›†ä¸Šè¯„ä¼°
    random_pred = random_search.predict(X_val)
    random_r2 = r2_score(y_val, random_pred)
    random_rmse = np.sqrt(mean_squared_error(y_val, random_pred))
    random_time = time.time() - start_time
    
    results.append({
        'method': 'éšæœºæœç´¢',
        'best_params': random_search.best_params_,
        'val_r2': random_r2,
        'val_rmse': random_rmse,
        'time': random_time
    })
    
    print(f"âœ… éšæœºæœç´¢å®Œæˆ!")
    print(f"   æœ€ä½³å‚æ•°: {random_search.best_params_}")
    print(f"   éªŒè¯é›†RÂ²: {random_r2:.4f}")
    print(f"   éªŒè¯é›†RMSE: {random_rmse:.4f}")
    print(f"   è€—æ—¶: {random_time:.1f}ç§’")
    
    # 7. é€‰æ‹©æœ€ä½³æ¨¡å‹å¹¶åœ¨æµ‹è¯•é›†ä¸Šè¯„ä¼°
    if grid_r2 > random_r2:
        best_model = grid_search.best_estimator_
        best_method = "ç½‘æ ¼æœç´¢"
        best_params = grid_search.best_params_
    else:
        best_model = random_search.best_estimator_
        best_method = "éšæœºæœç´¢"
        best_params = random_search.best_params_
    
    # æµ‹è¯•é›†è¯„ä¼°
    test_pred = best_model.predict(X_test)
    test_r2 = r2_score(y_test, test_pred)
    test_rmse = np.sqrt(mean_squared_error(y_test, test_pred))
    
    # 8. æ‰“å°æœ€ç»ˆç»“æœ
    print(f"\n" + "=" * 60)
    print(f"ğŸ† è¶…å‚æ•°ä¼˜åŒ–ç»“æœæ±‡æ€»")
    print(f"=" * 60)
    
    for result in results:
        print(f"\nğŸ“‹ {result['method']}:")
        print(f"   éªŒè¯é›†RÂ²: {result['val_r2']:.4f}")
        print(f"   éªŒè¯é›†RMSE: {result['val_rmse']:.4f}")
        print(f"   è€—æ—¶: {result['time']:.1f}ç§’")
        print(f"   æœ€ä½³å‚æ•°: {result['best_params']}")
    
    print(f"\nğŸ¯ æœ€ç»ˆæµ‹è¯•ç»“æœ (ä½¿ç”¨{best_method}çš„æœ€ä½³æ¨¡å‹):")
    print(f"   æµ‹è¯•é›†RÂ²: {test_r2:.4f}")
    print(f"   æµ‹è¯•é›†RMSE: {test_rmse:.4f}")
    print(f"   æœ€ä½³å‚æ•°: {best_params}")
    
    # 9. ç‰¹å¾é‡è¦æ€§
    feature_importance = pd.DataFrame({
        'feature': X.columns,
        'importance': best_model.feature_importances_
    }).sort_values('importance', ascending=False)
    
    print(f"\nğŸ“Š ç‰¹å¾é‡è¦æ€§æ’åº:")
    for _, row in feature_importance.iterrows():
        print(f"   {row['feature']}: {row['importance']:.4f}")
    
    print(f"\nâœ… è¶…å‚æ•°ä¼˜åŒ–æ¼”ç¤ºå®Œæˆ!")
    print(f"ğŸ“… å®Œæˆæ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    
    return results, best_model

def main():
    """ä¸»å‡½æ•° - ç‚¹å‡»è¿è¡ŒæŒ‰é’®æ‰§è¡Œæ­¤å‡½æ•°"""
    try:
        print("ğŸš€ å¯åŠ¨FRPè¶…å‚æ•°ä¼˜åŒ–æ¼”ç¤º...")
        results, best_model = run_hyperparameter_optimization()
        print(f"\nğŸ‰ æ¼”ç¤ºæˆåŠŸå®Œæˆ!")
        
    except Exception as e:
        print(f"âŒ æ¼”ç¤ºè¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    main()
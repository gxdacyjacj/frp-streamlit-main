#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
FRPè¶…å‚æ•°ä¼˜åŒ–å®éªŒè„šæœ¬
Hyperparameter Optimization Experiment Runner for FRP Models

ä½¿ç”¨7:2:1æ•°æ®åˆ†å‰²ç­–ç•¥è¿›è¡Œè¶…å‚æ•°ä¼˜åŒ–
"""

import sys
import os
import time
import json
import pandas as pd
import numpy as np
from pathlib import Path
from datetime import datetime
import warnings
warnings.filterwarnings('ignore')

# æ·»åŠ å½“å‰ç›®å½•åˆ°Pythonè·¯å¾„
sys.path.insert(0, str(Path(__file__).parent))

try:
    from data_loader import DataLoader
    from preprocessor import FRPDataPreprocessor
    from model_trainer import ModelTrainer
    from utils import print_model_performance
    from config import config
except ImportError as e:
    print(f"å¯¼å…¥æ¨¡å—å¤±è´¥: {e}")
    print("è¯·ç¡®ä¿æ‰€æœ‰æ¨¡å—æ–‡ä»¶å­˜åœ¨ä¸”å¯è®¿é—®")
    sys.exit(1)

class HyperparameterOptimizationExperiment:
    """å‚æ•°å®éªŒç®¡ç†å™¨"""
    
    def __init__(self):
        self.results = []
        self.experiment_id = datetime.now().strftime("%Y%m%d_%H%M%S")
        self.output_dir = Path("experiments") / f"hyperopt_{self.experiment_id}"
        self.output_dir.mkdir(parents=True, exist_ok=True)
        
    def define_parameter_sets(self):
        """å®šä¹‰ä¸åŒçš„å‚æ•°é…ç½®é›†åˆ"""
        
        parameter_sets = [
            # 1. ä¿å®ˆé…ç½® - å°æ¨¡å‹å¿«é€Ÿè®­ç»ƒ
            {
                'name': 'Conservative',
                'description': 'ä¿å®ˆå‚æ•°é…ç½®ï¼Œé€‚åˆå¿«é€ŸéªŒè¯',
                'params': {
                    'xgboost': {
                        'n_estimators': 100,
                        'max_depth': 4,
                        'learning_rate': 0.1,
                        'subsample': 0.8,
                        'colsample_bytree': 0.8,
                        'random_state': 42
                    },
                    'lightgbm': {
                        'n_estimators': 100,
                        'max_depth': 4,
                        'learning_rate': 0.1,
                        'subsample': 0.8,
                        'colsample_bytree': 0.8,
                        'random_state': 42,
                        'verbose': -1
                    },
                    'random_forest': {
                        'n_estimators': 100,
                        'max_depth': 4,
                        'random_state': 42,
                        'n_jobs': -1
                    }
                },
                'training_params': {
                    'test_size': 0.2,
                    'cv_folds': 3,
                    'enable_tuning': False
                }
            },
            
            # 2. æ ‡å‡†é…ç½® - å¹³è¡¡æ€§èƒ½å’Œé€Ÿåº¦
            {
                'name': 'Standard',
                'description': 'æ ‡å‡†å‚æ•°é…ç½®ï¼Œå¹³è¡¡æ€§èƒ½å’Œè®­ç»ƒæ—¶é—´',
                'params': {
                    'xgboost': {
                        'n_estimators': 200,
                        'max_depth': 6,
                        'learning_rate': 0.1,
                        'subsample': 0.8,
                        'colsample_bytree': 0.8,
                        'random_state': 42
                    },
                    'lightgbm': {
                        'n_estimators': 200,
                        'max_depth': 6,
                        'learning_rate': 0.1,
                        'subsample': 0.8,
                        'colsample_bytree': 0.8,
                        'random_state': 42,
                        'verbose': -1
                    },
                    'random_forest': {
                        'n_estimators': 200,
                        'max_depth': 6,
                        'random_state': 42,
                        'n_jobs': -1
                    }
                },
                'training_params': {
                    'test_size': 0.2,
                    'cv_folds': 5,
                    'enable_tuning': False
                }
            },
            
            # 3. é«˜æ€§èƒ½é…ç½® - è¿½æ±‚æœ€ä½³æ•ˆæœ
            {
                'name': 'High_Performance',
                'description': 'é«˜æ€§èƒ½å‚æ•°é…ç½®ï¼Œè¿½æ±‚æœ€ä½³æ¨¡å‹æ•ˆæœ',
                'params': {
                    'xgboost': {
                        'n_estimators': 500,
                        'max_depth': 8,
                        'learning_rate': 0.05,
                        'subsample': 0.9,
                        'colsample_bytree': 0.9,
                        'random_state': 42
                    },
                    'lightgbm': {
                        'n_estimators': 500,
                        'max_depth': 8,
                        'learning_rate': 0.05,
                        'subsample': 0.9,
                        'colsample_bytree': 0.9,
                        'random_state': 42,
                        'verbose': -1
                    },
                    'random_forest': {
                        'n_estimators': 500,
                        'max_depth': 8,
                        'random_state': 42,
                        'n_jobs': -1
                    }
                },
                'training_params': {
                    'test_size': 0.2,
                    'cv_folds': 5,
                    'enable_tuning': False
                }
            },
            
            # 4. æ·±åº¦å­¦ä¹ é£æ ¼é…ç½® - å¤æ‚æ¨¡å‹
            {
                'name': 'Deep_Learning_Style',
                'description': 'æ·±åº¦å­¦ä¹ é£æ ¼é…ç½®ï¼Œå¤æ‚æ¨¡å‹ç»“æ„',
                'params': {
                    'xgboost': {
                        'n_estimators': 1000,
                        'max_depth': 10,
                        'learning_rate': 0.01,
                        'subsample': 0.8,
                        'colsample_bytree': 0.8,
                        'reg_alpha': 0.1,
                        'reg_lambda': 0.1,
                        'random_state': 42
                    },
                    'lightgbm': {
                        'n_estimators': 1000,
                        'max_depth': 10,
                        'learning_rate': 0.01,
                        'subsample': 0.8,
                        'colsample_bytree': 0.8,
                        'reg_alpha': 0.1,
                        'reg_lambda': 0.1,
                        'random_state': 42,
                        'verbose': -1
                    },
                    'random_forest': {
                        'n_estimators': 1000,
                        'max_depth': 10,
                        'min_samples_split': 5,
                        'min_samples_leaf': 2,
                        'random_state': 42,
                        'n_jobs': -1
                    }
                },
                'training_params': {
                    'test_size': 0.2,
                    'cv_folds': 5,
                    'enable_tuning': False
                }
            },
            
            # 5. æ­£åˆ™åŒ–é…ç½® - é˜²æ­¢è¿‡æ‹Ÿåˆ
            {
                'name': 'Regularized',
                'description': 'æ­£åˆ™åŒ–é…ç½®ï¼Œé˜²æ­¢è¿‡æ‹Ÿåˆ',
                'params': {
                    'xgboost': {
                        'n_estimators': 300,
                        'max_depth': 5,
                        'learning_rate': 0.08,
                        'subsample': 0.7,
                        'colsample_bytree': 0.7,
                        'reg_alpha': 0.5,
                        'reg_lambda': 0.5,
                        'random_state': 42
                    },
                    'lightgbm': {
                        'n_estimators': 300,
                        'max_depth': 5,
                        'learning_rate': 0.08,
                        'subsample': 0.7,
                        'colsample_bytree': 0.7,
                        'reg_alpha': 0.5,
                        'reg_lambda': 0.5,
                        'random_state': 42,
                        'verbose': -1
                    },
                    'random_forest': {
                        'n_estimators': 300,
                        'max_depth': 5,
                        'min_samples_split': 10,
                        'min_samples_leaf': 5,
                        'max_features': 'sqrt',
                        'random_state': 42,
                        'n_jobs': -1
                    }
                },
                'training_params': {
                    'test_size': 0.25,
                    'cv_folds': 5,
                    'enable_tuning': False
                }
            },
            
            # 6. è¶…å‚æ•°ä¼˜åŒ–é…ç½® - è‡ªåŠ¨è°ƒå‚
            {
                'name': 'Auto_Tuning',
                'description': 'å¯ç”¨è¶…å‚æ•°è‡ªåŠ¨ä¼˜åŒ–',
                'params': {
                    'xgboost': {
                        'n_estimators': 200,
                        'max_depth': 6,
                        'learning_rate': 0.1,
                        'random_state': 42
                    },
                    'lightgbm': {
                        'n_estimators': 200,
                        'max_depth': 6,
                        'learning_rate': 0.1,
                        'random_state': 42,
                        'verbose': -1
                    },
                    'random_forest': {
                        'n_estimators': 200,
                        'max_depth': 6,
                        'random_state': 42,
                        'n_jobs': -1
                    }
                },
                'training_params': {
                    'test_size': 0.2,
                    'cv_folds': 3,  # å‡å°‘CVæŠ˜æ•°ä»¥åŠ å¿«è¶…å‚æ•°ä¼˜åŒ–
                    'enable_tuning': True
                }
            }
        ]
        
        return parameter_sets
    
    def run_single_experiment(self, param_set, data_df):
        """è¿è¡Œå•ä¸ªå‚æ•°é…ç½®å®éªŒ"""
        
        print(f"\n{'='*60}")
        print(f"å¼€å§‹å®éªŒ: {param_set['name']}")
        print(f"æè¿°: {param_set['description']}")
        print(f"{'='*60}")
        
        start_time = time.time()
        
        try:
            # ä¸´æ—¶ä¿®æ”¹é…ç½®
            original_model_params = config.MODEL_PARAMS.copy()
            original_test_size = config.TEST_SIZE
            original_cv_folds = config.CV_FOLDS
            
            # åº”ç”¨å®éªŒå‚æ•°
            config.MODEL_PARAMS.update(param_set['params'])
            config.TEST_SIZE = param_set['training_params']['test_size']
            config.CV_FOLDS = param_set['training_params']['cv_folds']
            
            # åˆ›å»ºæ¨¡å‹è®­ç»ƒå™¨
            trainer = ModelTrainer(
                enable_hyperparameter_tuning=param_set['training_params']['enable_tuning']
            )
            
            # è®­ç»ƒæ‰€æœ‰æ¨¡å‹
            training_results = trainer.train_all_models(
                data_df, 
                target_column=config.TARGET_VARIABLE
            )
            
            # è·å–æ€§èƒ½å¯¹æ¯”
            comparison_results = trainer.compare_models()
            
            # è®¡ç®—è®­ç»ƒæ—¶é—´
            training_time = time.time() - start_time
            
            # ä¿å­˜å®éªŒç»“æœ
            experiment_result = {
                'experiment_name': param_set['name'],
                'description': param_set['description'],
                'parameters': param_set['params'],
                'training_params': param_set['training_params'],
                'training_time_seconds': training_time,
                'model_results': training_results,
                'comparison_results': comparison_results,
                'timestamp': datetime.now().isoformat()
            }
            
            # æ¢å¤åŸå§‹é…ç½®
            config.MODEL_PARAMS = original_model_params
            config.TEST_SIZE = original_test_size
            config.CV_FOLDS = original_cv_folds
            
            return experiment_result
            
        except Exception as e:
            print(f"å®éªŒ {param_set['name']} å¤±è´¥: {e}")
            return {
                'experiment_name': param_set['name'],
                'error': str(e),
                'timestamp': datetime.now().isoformat()
            }
    
    def save_experiment_results(self, all_results):
        """ä¿å­˜å®éªŒç»“æœ"""
        
        # ä¿å­˜å®Œæ•´ç»“æœåˆ°JSON
        results_file = self.output_dir / "experiment_results.json"
        with open(results_file, 'w', encoding='utf-8') as f:
            json.dump(all_results, f, ensure_ascii=False, indent=2)
        
        # åˆ›å»ºç»“æœæ±‡æ€»è¡¨
        summary_data = []
        
        for result in all_results:
            if 'error' not in result:
                exp_name = result['experiment_name']
                training_time = result['training_time_seconds']
                
                # è·å–æ¯ä¸ªæ¨¡å‹çš„æœ€ä½³æ€§èƒ½
                for model_name, model_result in result['model_results'].items():
                    if 'test_metrics' in model_result:
                        metrics = model_result['test_metrics']
                        summary_data.append({
                            'Experiment': exp_name,
                            'Model': model_name,
                            'R2_Score': metrics.get('r2', 0),
                            'RMSE': metrics.get('rmse', float('inf')),
                            'MAE': metrics.get('mae', float('inf')),
                            'Training_Time_Sec': training_time,
                            'Hyperparameter_Tuning': result['training_params']['enable_tuning']
                        })
        
        # ä¿å­˜æ±‡æ€»è¡¨
        if summary_data:
            summary_df = pd.DataFrame(summary_data)
            summary_df = summary_df.sort_values('R2_Score', ascending=False)
            
            summary_file = self.output_dir / "experiment_summary.csv"
            summary_df.to_csv(summary_file, index=False, encoding='utf-8-sig')
            
            print(f"\nğŸ“Š å®éªŒç»“æœæ±‡æ€»:")
            print(summary_df.to_string(index=False))
            
            # æ‰¾å‡ºæœ€ä½³é…ç½®
            best_result = summary_df.iloc[0]
            print(f"\nğŸ† æœ€ä½³é…ç½®:")
            print(f"   å®éªŒåç§°: {best_result['Experiment']}")
            print(f"   æ¨¡å‹ç±»å‹: {best_result['Model']}")
            print(f"   R2å¾—åˆ†: {best_result['R2_Score']:.4f}")
            print(f"   RMSE: {best_result['RMSE']:.4f}")
            print(f"   è®­ç»ƒæ—¶é—´: {best_result['Training_Time_Sec']:.1f}ç§’")
            
        return self.output_dir
    
    def run_all_experiments(self):
        """è¿è¡Œæ‰€æœ‰å‚æ•°å®éªŒ"""
        
        print("ğŸš€ å¼€å§‹FRPæ¨¡å‹å‚æ•°å®éªŒ")
        print(f"å®éªŒID: {self.experiment_id}")
        print(f"ç»“æœä¿å­˜ç›®å½•: {self.output_dir}")
        
        # åŠ è½½æ•°æ®
        print("\nğŸ“Š åŠ è½½æ•°æ®...")
        loader = DataLoader("csv")
        df = loader.load_data()
        
        if df is None:
            print("æ•°æ®åŠ è½½å¤±è´¥ï¼")
            return
        
        # æ•°æ®é¢„å¤„ç†
        print("ğŸ”§ æ•°æ®é¢„å¤„ç†...")
        preprocessor = FRPDataPreprocessor()
        processed_df = preprocessor.create_model_dataset(df)
        
        if processed_df is None:
            print("æ•°æ®é¢„å¤„ç†å¤±è´¥ï¼")
            return
        
        print(f"é¢„å¤„ç†åæ•°æ®å½¢çŠ¶: {processed_df.shape}")
        
        # è·å–å‚æ•°é…ç½®é›†åˆ
        parameter_sets = self.define_parameter_sets()
        print(f"æ€»å…± {len(parameter_sets)} ä¸ªå®éªŒé…ç½®")
        
        # è¿è¡Œæ‰€æœ‰å®éªŒ
        all_results = []
        total_start_time = time.time()
        
        for i, param_set in enumerate(parameter_sets, 1):
            print(f"\nè¿›åº¦: {i}/{len(parameter_sets)}")
            result = self.run_single_experiment(param_set, processed_df)
            all_results.append(result)
        
        total_time = time.time() - total_start_time
        print(f"\nğŸ‰ æ‰€æœ‰å®éªŒå®Œæˆï¼æ€»ç”¨æ—¶: {total_time:.1f}ç§’")
        
        # ä¿å­˜ç»“æœ
        output_dir = self.save_experiment_results(all_results)
        print(f"\nğŸ“ ç»“æœå·²ä¿å­˜åˆ°: {output_dir}")
        
        return all_results, output_dir

def main():
    """ä¸»å‡½æ•°"""
    
    print("FRPé’¢ç­‹è€ä¹…æ€§é¢„æµ‹ - è¶…å‚æ•°ä¼˜åŒ–å®éªŒ")
    print("="*50)
    
    # åˆ›å»ºå®éªŒç®¡ç†å™¨
    experiment = HyperparameterOptimizationExperiment()
    
    # è¿è¡Œæ‰€æœ‰å®éªŒ
    results, output_dir = experiment.run_all_experiments()
    
    print(f"\nâœ… è¶…å‚æ•°ä¼˜åŒ–å®éªŒå®Œæˆï¼")
    print(f"è¯¦ç»†ç»“æœè¯·æŸ¥çœ‹: {output_dir}")
    print(f"- hyperopt_results.json: å®Œæ•´å®éªŒæ•°æ®")
    print(f"- hyperopt_summary.csv: ç»“æœæ±‡æ€»è¡¨")

if __name__ == "__main__":
    main()
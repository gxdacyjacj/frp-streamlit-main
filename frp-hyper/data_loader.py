# -*- coding: utf-8 -*-
"""
FRP é’¢ç­‹è€ä¹…æ€§é¢„æµ‹ - æ•°æ®åŠ è½½æ¨¡å—
Data Loading Module for FRP Rebar Durability Prediction

æ”¯æŒä»CSVæ–‡ä»¶å’Œæ•°æ®åº“åŠ è½½æ•°æ®
"""

import pandas as pd
import numpy as np
import os
from pathlib import Path
from typing import Optional, Union, Dict, Any
import warnings

# å¯é€‰çš„æ•°æ®åº“æ”¯æŒ
try:
    import pymysql
    from sqlalchemy import create_engine, text
    DATABASE_AVAILABLE = True
except ImportError:
    DATABASE_AVAILABLE = False
    print("Warning: Database dependencies not available. CSV loading only.")

try:
    from .config import config
except ImportError:
    # å¦‚æœç›¸å¯¹å¯¼å…¥å¤±è´¥ï¼Œå°è¯•ç»å¯¹å¯¼å…¥
    try:
        from config import config
    except ImportError:
        print("Warning: Could not import config module")

class DataLoader:
    """æ•°æ®åŠ è½½å™¨ - æ”¯æŒå¤šç§æ•°æ®æº"""
    
    def __init__(self, data_source: str = "csv"):
        """
        åˆå§‹åŒ–æ•°æ®åŠ è½½å™¨
        
        Args:
            data_source: æ•°æ®æºç±»å‹ ('csv' æˆ– 'database')
        """
        self.data_source = data_source
        self.engine = None
        
        if data_source == "database" and DATABASE_AVAILABLE:
            self._init_database_connection()
    
    def _init_database_connection(self):
        """åˆå§‹åŒ–æ•°æ®åº“è¿æ¥"""
        try:
            db_config = config.DATABASE_CONFIG
            connection_string = (
                f"mysql+pymysql://{db_config['user']}:{db_config['password']}"
                f"@{db_config['host']}:{db_config['port']}/{db_config['database']}"
            )
            
            self.engine = create_engine(
                connection_string,
                pool_size=5,
                max_overflow=10,
                pool_timeout=30,
                pool_recycle=3600
            )
            
            # æµ‹è¯•è¿æ¥
            with self.engine.connect() as conn:
                conn.execute(text("SELECT 1"))
            
            print("âœ… Database connection established")
            
        except Exception as e:
            print(f"âŒ Database connection failed: {e}")
            self.engine = None
    
    def load_data(self, 
                  source_path: Optional[str] = None,
                  table_name: str = "research_data",
                  **kwargs) -> Optional[pd.DataFrame]:
        """
        åŠ è½½æ•°æ®
        
        Args:
            source_path: CSVæ–‡ä»¶è·¯å¾„ï¼ˆå½“ä½¿ç”¨CSVæ—¶ï¼‰
            table_name: æ•°æ®åº“è¡¨åï¼ˆå½“ä½¿ç”¨æ•°æ®åº“æ—¶ï¼‰
            **kwargs: å…¶ä»–å‚æ•°
            
        Returns:
            pd.DataFrame: åŠ è½½çš„æ•°æ®
        """
        
        if self.data_source == "csv":
            return self._load_from_csv(source_path, **kwargs)
        elif self.data_source == "database":
            return self._load_from_database(table_name, **kwargs)
        else:
            raise ValueError(f"Unsupported data source: {self.data_source}")
    
    def _load_from_csv(self, file_path: Optional[str] = None, **kwargs) -> Optional[pd.DataFrame]:
        """ä»CSVæˆ–Excelæ–‡ä»¶åŠ è½½æ•°æ®"""
        
        if file_path is None:
            file_path = config.DEFAULT_DATA_FILE
        
        try:
            if not os.path.exists(file_path):
                print(f"âŒ Data file not found: {file_path}")
                return None
            
            # æ£€æµ‹æ–‡ä»¶ç±»å‹
            file_ext = Path(file_path).suffix.lower()
            
            if file_ext == '.xlsx' or file_ext == '.xls':
                # è¯»å–Excelæ–‡ä»¶ï¼Œè·³è¿‡å‰3è¡Œæ ‡é¢˜
                print(f"Loading Excel file: {file_path}")
                df = pd.read_excel(file_path, skiprows=3, **kwargs)
                file_type = "Excel"
            elif file_ext == '.csv':
                # è¯»å–CSVæ–‡ä»¶
                print(f"Loading CSV file: {file_path}")
                df = pd.read_csv(file_path, **kwargs)
                file_type = "CSV"
            else:
                print(f"Unsupported file format: {file_ext}")
                return None
            
            print(f"Successfully loaded data from {file_type}: {df.shape}")
            print(f"   Columns: {list(df.columns[:10])}{'...' if len(df.columns) > 10 else ''}")
            
            # åº”ç”¨æ•°æ®è¿‡æ»¤å’Œæ¸…ç†
            cleaned_df = self._apply_data_filtering(df)
            
            return self._basic_data_cleaning(cleaned_df)
            
        except Exception as e:
            print(f"Failed to load data file: {e}")
            return None
    
    def _load_from_database(self, table_name: str = "research_data", **kwargs) -> Optional[pd.DataFrame]:
        """ä»æ•°æ®åº“åŠ è½½æ•°æ®"""
        
        if not DATABASE_AVAILABLE:
            print("âŒ Database functionality not available")
            return None
            
        if self.engine is None:
            print("âŒ Database connection not established")
            return None
        
        try:
            # æ£€æŸ¥è¡¨æ˜¯å¦å­˜åœ¨
            with self.engine.connect() as conn:
                result = conn.execute(text(f"SHOW TABLES LIKE '{table_name}'")).fetchone()
                if not result:
                    print(f"âŒ Table '{table_name}' does not exist")
                    return None
            
            # æŸ¥è¯¢æ•°æ®
            query = f"SELECT * FROM {table_name}"
            df = pd.read_sql(query, self.engine, **kwargs)
            
            print(f"âœ… Successfully loaded data from database: {df.shape}")
            print(f"   Table: {table_name}")
            print(f"   Columns: {list(df.columns)}")
            
            return self._basic_data_cleaning(df)
            
        except Exception as e:
            print(f"âŒ Failed to load from database: {e}")
            return None
    
    def _apply_data_filtering(self, df: pd.DataFrame) -> pd.DataFrame:
        """åº”ç”¨æ•°æ®è¿‡æ»¤è§„åˆ™"""
        
        print("Applying data filtering rules...")
        
        # è®°å½•åŸå§‹å½¢çŠ¶
        original_shape = df.shape
        
        # æ£€æŸ¥ç¬¬ä¸€åˆ—æ˜¯å¦å­˜åœ¨
        if df.empty or df.shape[1] == 0:
            print("Warning: No data to filter")
            return df
        
        first_col = df.iloc[:, 0]
        print(f"   First column name: {df.columns[0]}")
        print(f"   First column data type: {first_col.dtype}")
        
        # åˆ†æç¬¬ä¸€åˆ—çš„å€¼åˆ†å¸ƒ
        print("   First column value distribution:")
        try:
            value_counts = first_col.value_counts()
            print(f"     {dict(value_counts.head(10))}")
        except Exception as e:
            print(f"     Error analyzing distribution: {e}")
        
        # è½¬æ¢ç¬¬ä¸€åˆ—ä¸ºæ•°å€¼ç±»å‹ä»¥ä¾¿è¿‡æ»¤
        numeric_first_col = pd.to_numeric(first_col, errors='coerce')
        
        # è¿‡æ»¤è§„åˆ™ï¼šåªä¿ç•™ç¬¬ä¸€åˆ—å€¼ä¸º1çš„è¡Œ
        valid_mask = (numeric_first_col == 1)
        filtered_df = df[valid_mask].copy()
        
        # ç»Ÿè®¡è¿‡æ»¤ç»“æœ
        total_rows = len(df)
        valid_rows = valid_mask.sum()
        invalid_rows = len(df) - valid_rows
        
        print(f"   Total rows: {total_rows}")
        print(f"   Valid rows (first_col=1): {valid_rows}")
        print(f"   Filtered out rows (first_colâ‰ 1): {invalid_rows}")
        print(f"   Retention rate: {valid_rows/total_rows*100:.1f}%")
        
        if valid_rows == 0:
            print("Warning: No valid rows found after filtering!")
            return df  # è¿”å›åŸå§‹æ•°æ®ä»¥é¿å…ç©ºæ•°æ®é›†
        
        return filtered_df

    def _basic_data_cleaning(self, df: pd.DataFrame) -> pd.DataFrame:
        """åŸºç¡€æ•°æ®æ¸…ç†"""
        
        print("Performing basic data cleaning...")
        
        # è®°å½•åŸå§‹å½¢çŠ¶
        original_shape = df.shape
        
        # 1. ç§»é™¤å®Œå…¨ç©ºçš„è¡Œå’Œåˆ—
        df = df.dropna(how='all')  # ç§»é™¤å…¨ç©ºè¡Œ
        df = df.dropna(axis=1, how='all')  # ç§»é™¤å…¨ç©ºåˆ—
        
        # 2. æ¸…ç†åˆ—å
        df.columns = df.columns.str.strip()  # ç§»é™¤å‰åç©ºæ ¼
        
        # 3. å¤„ç†æ˜æ˜¾çš„ç¼ºå¤±å€¼æ ‡è®°
        missing_markers = ['SMD', 'smd', 'Notreported', 'not reported', 'Not reported', 'NOT REPORTED']
        for marker in missing_markers:
            df = df.replace(marker, np.nan)
        
        # 4. æ•°æ®ç±»å‹ä¼˜åŒ–
        for col in df.columns:
            # å°è¯•å°†objectç±»å‹çš„æ•°å€¼åˆ—è½¬æ¢ä¸ºæ•°å€¼ç±»å‹
            if df[col].dtype == 'object':
                # æ£€æŸ¥æ˜¯å¦å¯ä»¥è½¬æ¢ä¸ºæ•°å€¼
                try:
                    numeric_series = pd.to_numeric(df[col], errors='coerce')
                    # å¦‚æœè½¬æ¢åéç©ºå€¼æ¯”ä¾‹å¤§äº50%ï¼Œåˆ™è®¤ä¸ºæ˜¯æ•°å€¼åˆ—
                    if numeric_series.notna().sum() / len(df) > 0.5:
                        df[col] = numeric_series
                except:
                    pass
        
        print(f"   Original shape: {original_shape}")
        print(f"   Cleaned shape:  {df.shape}")
        print(f"   Removed {original_shape[0] - df.shape[0]} rows")
        print(f"   Removed {original_shape[1] - df.shape[1]} columns")
        
        return df
    
    def save_processed_data(self, df: pd.DataFrame, 
                           output_path: Optional[str] = None) -> bool:
        """ä¿å­˜é¢„å¤„ç†åçš„æ•°æ®"""
        
        if output_path is None:
            output_path = config.PROCESSED_DATA_FILE
        
        try:
            # ç¡®ä¿ç›®å½•å­˜åœ¨
            os.makedirs(os.path.dirname(output_path), exist_ok=True)
            
            # ä¿å­˜æ•°æ®
            df.to_csv(output_path, index=False, encoding='utf-8')
            
            print(f"âœ… Processed data saved: {output_path}")
            return True
            
        except Exception as e:
            print(f"âŒ Failed to save processed data: {e}")
            return False
    
    def get_data_info(self, df: pd.DataFrame) -> Dict[str, Any]:
        """è·å–æ•°æ®é›†ä¿¡æ¯"""
        
        info = {
            'shape': df.shape,
            'columns': list(df.columns),
            'dtypes': df.dtypes.to_dict(),
            'missing_values': df.isnull().sum().to_dict(),
            'memory_usage': df.memory_usage(deep=True).sum(),
            'numeric_columns': list(df.select_dtypes(include=[np.number]).columns),
            'categorical_columns': list(df.select_dtypes(include=['object']).columns)
        }
        
        return info
    
    def print_data_summary(self, df: pd.DataFrame):
        """æ‰“å°æ•°æ®æ‘˜è¦"""
        
        info = self.get_data_info(df)
        
        print("\nğŸ“Š Data Summary:")
        print("=" * 50)
        print(f"Shape: {info['shape']}")
        print(f"Memory Usage: {info['memory_usage'] / 1024 / 1024:.2f} MB")
        
        print(f"\nNumeric Columns ({len(info['numeric_columns'])}):")
        for col in info['numeric_columns'][:10]:  # æ˜¾ç¤ºå‰10ä¸ª
            missing = info['missing_values'][col]
            print(f"  - {col}: {missing} missing values")
        
        print(f"\nCategorical Columns ({len(info['categorical_columns'])}):")
        for col in info['categorical_columns'][:10]:  # æ˜¾ç¤ºå‰10ä¸ª
            missing = info['missing_values'][col]
            print(f"  - {col}: {missing} missing values")
        
        print("=" * 50)

# ä¾¿æ·å‡½æ•°
def load_default_data() -> Optional[pd.DataFrame]:
    """åŠ è½½é»˜è®¤æ•°æ®"""
    loader = DataLoader("csv")
    return loader.load_data()

def load_data_from_csv(file_path: str) -> Optional[pd.DataFrame]:
    """ä»æŒ‡å®šCSVæ–‡ä»¶åŠ è½½æ•°æ®"""
    loader = DataLoader("csv")
    return loader.load_data(file_path)

def load_data_from_database(table_name: str = "research_data") -> Optional[pd.DataFrame]:
    """ä»æ•°æ®åº“åŠ è½½æ•°æ®"""
    loader = DataLoader("database")
    return loader.load_data(table_name=table_name)
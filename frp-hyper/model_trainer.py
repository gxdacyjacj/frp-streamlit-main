# -*- coding: utf-8 -*-
"""
FRP é’¢ç­‹è€ä¹…æ€§é¢„æµ‹ - æ¨¡å‹è®­ç»ƒæ¨¡å—
Model Training Module for FRP Rebar Durability Prediction

åŒ…å«ï¼š
- å¤šç§æœºå™¨å­¦ä¹ ç®—æ³•
- è¶…å‚æ•°ä¼˜åŒ–
- æ¨¡å‹è¯„ä¼°å’ŒéªŒè¯
"""

import pandas as pd
import numpy as np
import pickle
from pathlib import Path
from typing import Dict, List, Tuple, Optional, Any
import warnings
warnings.filterwarnings('ignore')

# æœºå™¨å­¦ä¹ ç›¸å…³
from sklearn.model_selection import train_test_split, GridSearchCV, RandomizedSearchCV, cross_val_score, RepeatedKFold
from sklearn.ensemble import RandomForestRegressor, VotingRegressor
from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error
from sklearn.pipeline import Pipeline

# å¯é€‰çš„é«˜çº§ç®—æ³•
try:
    from xgboost import XGBRegressor
    XGBOOST_AVAILABLE = True
except ImportError:
    XGBOOST_AVAILABLE = False
    print("âš ï¸ XGBoost not available")

try:
    from lightgbm import LGBMRegressor
    LIGHTGBM_AVAILABLE = True
except ImportError:
    LIGHTGBM_AVAILABLE = False
    print("âš ï¸ LightGBM not available")

try:
    from .config import config
    from .utils import create_enhanced_preprocessor, diagnose_model_performance, print_model_performance, save_model_safely
    from .preprocessor import FRPDataPreprocessor
except ImportError:
    from config import config
    from utils import create_enhanced_preprocessor, diagnose_model_performance, print_model_performance, save_model_safely
    from preprocessor import FRPDataPreprocessor

class ModelTrainer:
    """æ¨¡å‹è®­ç»ƒå™¨ - æ”¯æŒå¤šç§ç®—æ³•å’Œè‡ªåŠ¨åŒ–è®­ç»ƒæµç¨‹"""
    
    def __init__(self, enable_hyperparameter_tuning: bool = True):
        """
        åˆå§‹åŒ–æ¨¡å‹è®­ç»ƒå™¨
        
        Args:
            enable_hyperparameter_tuning: æ˜¯å¦å¯ç”¨è¶…å‚æ•°ä¼˜åŒ–
        """
        self.enable_hyperparameter_tuning = enable_hyperparameter_tuning
        self.models = {}
        self.trained_models = {}
        self.evaluation_results = {}
        self.feature_info = None
        
        # åˆå§‹åŒ–æ”¯æŒçš„æ¨¡å‹
        self._init_models()
    
    def _init_models(self):
        """åˆå§‹åŒ–æ”¯æŒçš„æ¨¡å‹"""
        
        # Random Forestï¼ˆå§‹ç»ˆå¯ç”¨ï¼‰
        self.models['random_forest'] = RandomForestRegressor(
            **config.get_model_params('random_forest')
        )
        
        # XGBoostï¼ˆå¦‚æœå¯ç”¨ï¼‰
        if XGBOOST_AVAILABLE:
            self.models['xgboost'] = XGBRegressor(
                **config.get_model_params('xgboost')
            )
        
        # LightGBMï¼ˆå¦‚æœå¯ç”¨ï¼‰
        if LIGHTGBM_AVAILABLE:
            self.models['lightgbm'] = LGBMRegressor(
                **config.get_model_params('lightgbm')
            )
        
        print(f"âœ… Initialized {len(self.models)} models: {list(self.models.keys())}")
    
    def prepare_train_val_test_splits(self, X: np.ndarray, y: np.ndarray) -> Tuple[np.ndarray, np.ndarray, np.ndarray, np.ndarray, np.ndarray, np.ndarray]:
        """
        å‡†å¤‡7:2:1æ•°æ®åˆ†å‰²ï¼ˆè®­ç»ƒ:éªŒè¯:æµ‹è¯•ï¼‰
        
        Args:
            X: ç‰¹å¾çŸ©é˜µ
            y: ç›®æ ‡å˜é‡
            
        Returns:
            X_train, X_val, X_test, y_train, y_val, y_test
        """
        # é¦–å…ˆåˆ†ç¦»å‡ºæµ‹è¯•é›†(10%)
        X_temp, X_test, y_temp, y_test = train_test_split(
            X, y, test_size=config.TEST_SIZE, random_state=42
        )
        
        # ä»å‰©ä½™90%ä¸­åˆ†ç¦»è®­ç»ƒé›†(77.8%â‰ˆ70%)å’ŒéªŒè¯é›†(22.2%â‰ˆ20%)
        val_ratio = config.VALIDATION_SIZE / (1 - config.TEST_SIZE)  # 0.2/0.9 â‰ˆ 0.222
        X_train, X_val, y_train, y_val = train_test_split(
            X_temp, y_temp, test_size=val_ratio, random_state=42
        )
        
        print(f"ğŸ“Š æ•°æ®åˆ†å‰²å®Œæˆ:")
        print(f"   è®­ç»ƒé›†: {len(X_train)} æ ·æœ¬ ({len(X_train)/len(X)*100:.1f}%)")
        print(f"   éªŒè¯é›†: {len(X_val)} æ ·æœ¬ ({len(X_val)/len(X)*100:.1f}%)")
        print(f"   æµ‹è¯•é›†: {len(X_test)} æ ·æœ¬ ({len(X_test)/len(X)*100:.1f}%)")
        
        return X_train, X_val, X_test, y_train, y_val, y_test
        
    def hyperparameter_search(self, model_name: str, X_train: np.ndarray, y_train: np.ndarray, 
                            X_val: np.ndarray, y_val: np.ndarray) -> object:
        """
        ä½¿ç”¨éªŒè¯é›†è¿›è¡Œè¶…å‚æ•°æœç´¢
        
        Args:
            model_name: æ¨¡å‹åç§°
            X_train: è®­ç»ƒç‰¹å¾
            y_train: è®­ç»ƒç›®æ ‡
            X_val: éªŒè¯ç‰¹å¾  
            y_val: éªŒè¯ç›®æ ‡
            
        Returns:
            ä¼˜åŒ–åçš„æ¨¡å‹
        """
        if not config.ENABLE_HYPERPARAMETER_TUNING:
            return self.models[model_name]
        
        if model_name not in config.HYPERPARAMETER_SPACES:
            print(f"âš ï¸ No hyperparameter space defined for {model_name}, using default parameters")
            return self.models[model_name]
        
        print(f"ğŸ” å¼€å§‹{model_name}è¶…å‚æ•°ä¼˜åŒ–...")
        
        base_model = self.models[model_name]
        param_space = config.HYPERPARAMETER_SPACES[model_name]
        
        # åˆ›å»ºé¢„å¤„ç†å™¨
        if self.feature_info:
            preprocessor = create_enhanced_preprocessor(
                categorical_cols=self.feature_info['categorical_features'],
                numeric_cols=self.feature_info['numeric_features']
            )
            
            # é¢„å¤„ç†è®­ç»ƒå’ŒéªŒè¯æ•°æ®
            X_train_processed = preprocessor.fit_transform(X_train)
            X_val_processed = preprocessor.transform(X_val)
        else:
            X_train_processed = X_train
            X_val_processed = X_val
        
        # é€‰æ‹©æœç´¢æ–¹æ³•
        if config.HYPERPARAMETER_SEARCH_METHOD == 'grid':
            search = GridSearchCV(
                base_model, 
                param_space,
                cv=config.TUNING_CV_FOLDS,
                scoring='r2',
                n_jobs=-1,
                verbose=1
            )
        elif config.HYPERPARAMETER_SEARCH_METHOD == 'random':
            search = RandomizedSearchCV(
                base_model,
                param_space, 
                n_iter=config.TUNING_N_ITER,
                cv=config.TUNING_CV_FOLDS,
                scoring='r2',
                n_jobs=-1,
                random_state=42,
                verbose=1
            )
        else:
            print(f"âš ï¸ Unsupported search method: {config.HYPERPARAMETER_SEARCH_METHOD}")
            return base_model
        
        # æ‰§è¡Œæœç´¢
        search.fit(X_train_processed, y_train)
        
        # åœ¨éªŒè¯é›†ä¸Šè¯„ä¼°æœ€ä½³æ¨¡å‹
        best_model = search.best_estimator_
        val_score = best_model.score(X_val_processed, y_val)
        
        print(f"âœ… è¶…å‚æ•°ä¼˜åŒ–å®Œæˆ")
        print(f"   æœ€ä½³å‚æ•°: {search.best_params_}")
        print(f"   éªŒè¯é›†RÂ²: {val_score:.4f}")
        
        return best_model
    
    def prepare_data(self, df: pd.DataFrame, target_column: str = None) -> Tuple[np.ndarray, np.ndarray, Dict]:
        """
        å‡†å¤‡è®­ç»ƒæ•°æ®
        
        Args:
            df: é¢„å¤„ç†åçš„æ•°æ®
            target_column: ç›®æ ‡å˜é‡åˆ—å
            
        Returns:
            X, y, feature_info
        """
        
        if target_column is None:
            target_column = config.TARGET_VARIABLE
            # å°è¯•åŒ¹é…å¯èƒ½çš„ç›®æ ‡åˆ—å
            possible_targets = ['Tensile strength retention', 'Tensile_strength_retention', 'retention1']
            for col in possible_targets:
                if col in df.columns:
                    target_column = col
                    break
        
        if target_column not in df.columns:
            raise ValueError(f"Target column '{target_column}' not found in data")
        
        print(f"ğŸ¯ Using target variable: {target_column}")
        
        # åˆ†ç¦»ç‰¹å¾å’Œç›®æ ‡å˜é‡
        feature_columns = [col for col in df.columns if col not in [target_column, 'Title']]
        X = df[feature_columns]
        y = df[target_column]
        
        # ç§»é™¤ç›®æ ‡å˜é‡ä¸ºç©ºçš„è¡Œ
        valid_mask = y.notna()
        X = X[valid_mask]
        y = y[valid_mask]
        
        print(f"ğŸ“Š Data shape after removing missing targets: X={X.shape}, y={y.shape}")
        
        # åˆ†ç¦»æ•°å€¼å’Œåˆ†ç±»ç‰¹å¾
        numeric_features = []
        categorical_features = []
        
        for col in X.columns:
            if X[col].dtype in ['int64', 'float64']:
                # æ£€æŸ¥æ˜¯å¦ä¸ºäºŒè¿›åˆ¶åˆ†ç±»ç‰¹å¾
                unique_values = X[col].dropna().unique()
                if len(unique_values) <= 2 and all(v in [0, 1] for v in unique_values if not pd.isna(v)):
                    categorical_features.append(col)
                else:
                    numeric_features.append(col)
            else:
                categorical_features.append(col)
        
        # ä¿å­˜ç‰¹å¾ä¿¡æ¯
        self.feature_info = {
            'feature_columns': feature_columns,
            'numeric_features': numeric_features,
            'categorical_features': categorical_features,
            'target_variable': target_column,
            'feature_names': list(X.columns)
        }
        
        print(f"ğŸ“ˆ Feature analysis:")
        print(f"   - Numeric features: {len(numeric_features)}")
        print(f"   - Categorical features: {len(categorical_features)}")
        print(f"   - Total features: {len(feature_columns)}")
        
        return X.values, y.values, self.feature_info
    
    def train_model_with_hyperopt(self, model_name: str, X: np.ndarray, y: np.ndarray) -> Dict[str, Any]:
        """
        ä½¿ç”¨è¶…å‚æ•°ä¼˜åŒ–è®­ç»ƒå•ä¸ªæ¨¡å‹ï¼ˆ7:2:1åˆ†å‰²ï¼‰
        
        Args:
            model_name: æ¨¡å‹åç§°
            X: ç‰¹å¾æ•°æ®
            y: ç›®æ ‡å˜é‡
            
        Returns:
            è®­ç»ƒç»“æœ
        """
        
        if model_name not in self.models:
            raise ValueError(f"Model '{model_name}' not supported. Available: {list(self.models.keys())}")
        
        print(f"ğŸš€ Training {model_name} with hyperparameter optimization...")
        
        # ä½¿ç”¨7:2:1åˆ†å‰²æ•°æ®
        X_train, X_val, X_test, y_train, y_val, y_test = self.prepare_train_val_test_splits(X, y)
        
        try:
            # è¶…å‚æ•°ä¼˜åŒ–
            optimized_model = self.hyperparameter_search(model_name, X_train, y_train, X_val, y_val)
            
            # åˆ›å»ºé¢„å¤„ç†å™¨
            if self.feature_info:
                preprocessor = create_enhanced_preprocessor(
                    categorical_cols=self.feature_info['categorical_features'],
                    numeric_cols=self.feature_info['numeric_features']
                )
                
                # åˆ›å»ºç®¡é“
                pipeline = Pipeline([
                    ('preprocessor', preprocessor),
                    ('regressor', optimized_model)
                ])
                
                # åœ¨è®­ç»ƒ+éªŒè¯é›†ä¸Šé‡æ–°è®­ç»ƒæœ€ç»ˆæ¨¡å‹
                X_train_val = np.vstack([X_train, X_val])
                y_train_val = np.concatenate([y_train, y_val])
                
                pipeline.fit(X_train_val, y_train_val)
                
                # åœ¨æµ‹è¯•é›†ä¸Šè¯„ä¼°
                y_pred = pipeline.predict(X_test)
                
                # ä¿å­˜è®­ç»ƒå¥½çš„æ¨¡å‹
                self.trained_models[model_name] = pipeline
                
            else:
                # æ²¡æœ‰ç‰¹å¾ä¿¡æ¯æ—¶çš„ç®€å•å¤„ç†
                X_train_val = np.vstack([X_train, X_val])
                y_train_val = np.concatenate([y_train, y_val])
                
                optimized_model.fit(X_train_val, y_train_val)
                y_pred = optimized_model.predict(X_test)
                
                # ä¿å­˜è®­ç»ƒå¥½çš„æ¨¡å‹
                self.trained_models[model_name] = optimized_model
            
            # è®¡ç®—è¯„ä¼°æŒ‡æ ‡
            r2 = r2_score(y_test, y_pred)
            mse = mean_squared_error(y_test, y_pred)
            mae = mean_absolute_error(y_test, y_pred)
            rmse = np.sqrt(mse)
            
            result = {
                'model_name': model_name,
                'r2_score': r2,
                'mse': mse,
                'mae': mae,
                'rmse': rmse,
                'test_size': len(y_test),
                'train_size': len(y_train),
                'val_size': len(y_val),
                'hyperparameter_optimized': config.ENABLE_HYPERPARAMETER_TUNING
            }
            
            # ä¿å­˜ç»“æœ
            self.evaluation_results[model_name] = result
            
            # æ‰“å°ç»“æœ
            print(f"âœ… {model_name} training completed!")
            print(f"   RÂ² Score: {r2:.4f}")
            print(f"   RMSE: {rmse:.4f}")
            print(f"   Test samples: {len(y_test)}")
            
            return result
            
        except Exception as e:
            print(f"âŒ {model_name} training failed: {str(e)}")
            return {
                'model_name': model_name,
                'error': str(e),
                'status': 'failed'
            }
    
    def train_model(self, model_name: str, X: np.ndarray, y: np.ndarray, 
                   test_size: float = None) -> Dict[str, Any]:
        """
        è®­ç»ƒå•ä¸ªæ¨¡å‹
        
        Args:
            model_name: æ¨¡å‹åç§°
            X: ç‰¹å¾æ•°æ®
            y: ç›®æ ‡å˜é‡
            test_size: æµ‹è¯•é›†æ¯”ä¾‹
            
        Returns:
            è®­ç»ƒç»“æœ
        """
        
        if model_name not in self.models:
            raise ValueError(f"Model '{model_name}' not supported. Available: {list(self.models.keys())}")
        
        if test_size is None:
            test_size = config.TEST_SIZE
        
        print(f"ğŸš€ Training {model_name}...")
        
        # åˆ†å‰²æ•°æ®
        X_train, X_test, y_train, y_test = train_test_split(
            X, y, test_size=test_size, random_state=42
        )
        
        # åˆ›å»ºé¢„å¤„ç†å™¨
        if self.feature_info:
            preprocessor = create_enhanced_preprocessor(
                categorical_cols=self.feature_info['categorical_features'],
                numeric_cols=self.feature_info['numeric_features']
            )
            
            # åˆ›å»ºç®¡é“
            model = self.models[model_name]
            pipeline = Pipeline([
                ('preprocessor', preprocessor),
                ('regressor', model)
            ])
        else:
            # å¦‚æœæ²¡æœ‰ç‰¹å¾ä¿¡æ¯ï¼Œç›´æ¥ä½¿ç”¨æ¨¡å‹
            pipeline = self.models[model_name]
        
        # è¶…å‚æ•°ä¼˜åŒ–
        if self.enable_hyperparameter_tuning:
            pipeline = self._optimize_hyperparameters(pipeline, X_train, y_train, model_name)
        
        # è®­ç»ƒæ¨¡å‹
        pipeline.fit(X_train, y_train)
        
        # é¢„æµ‹
        y_pred_train = pipeline.predict(X_train)
        y_pred_test = pipeline.predict(X_test)
        
        # è¯„ä¼°
        train_metrics = diagnose_model_performance(y_train, y_pred_train, f"{model_name}_train")
        test_metrics = diagnose_model_performance(y_test, y_pred_test, f"{model_name}_test")
        
        # äº¤å‰éªŒè¯
        cv_scores = cross_val_score(pipeline, X_train, y_train, 
                                   cv=config.CV_FOLDS, scoring='r2')
        
        # ä¿å­˜è®­ç»ƒç»“æœ
        result = {
            'model': pipeline,
            'train_metrics': train_metrics,
            'test_metrics': test_metrics,
            'cv_scores': cv_scores,
            'cv_mean': cv_scores.mean(),
            'cv_std': cv_scores.std(),
            'feature_info': self.feature_info
        }
        
        self.trained_models[model_name] = result
        
        # æ‰“å°ç»“æœ
        print_model_performance(test_metrics)
        print(f"CV Score: {cv_scores.mean():.4f} Â± {cv_scores.std():.4f}")
        
        return result
    
    def _optimize_hyperparameters(self, pipeline, X_train, y_train, model_name):
        """è¶…å‚æ•°ä¼˜åŒ–"""
        
        print(f"ğŸ”§ Optimizing hyperparameters for {model_name}...")
        
        # å®šä¹‰æœç´¢ç©ºé—´
        param_grids = {
            'random_forest': {
                'regressor__n_estimators': [100, 200, 300],
                'regressor__max_depth': [3, 6, 10, None],
                'regressor__min_samples_split': [2, 5, 10],
                'regressor__min_samples_leaf': [1, 2, 4]
            },
            'xgboost': {
                'regressor__n_estimators': [100, 200, 300],
                'regressor__max_depth': [3, 6, 9],
                'regressor__learning_rate': [0.01, 0.1, 0.2],
                'regressor__subsample': [0.8, 0.9, 1.0]
            },
            'lightgbm': {
                'regressor__n_estimators': [100, 200, 300],
                'regressor__max_depth': [3, 6, 9],
                'regressor__learning_rate': [0.01, 0.1, 0.2],
                'regressor__subsample': [0.8, 0.9, 1.0]
            }
        }
        
        param_grid = param_grids.get(model_name, {})
        
        if param_grid:
            grid_search = GridSearchCV(
                pipeline, param_grid,
                cv=config.TUNING_CV_FOLDS,
                scoring='r2',
                n_jobs=-1,
                verbose=0
            )
            
            grid_search.fit(X_train, y_train)
            
            print(f"   Best params: {grid_search.best_params_}")
            print(f"   Best CV score: {grid_search.best_score_:.4f}")
            
            return grid_search.best_estimator_
        
        return pipeline
    
    def train_all_models(self, df: pd.DataFrame, target_column: str = None) -> Dict[str, Any]:
        """
        è®­ç»ƒæ‰€æœ‰å¯ç”¨æ¨¡å‹
        
        Args:
            df: é¢„å¤„ç†åçš„æ•°æ®
            target_column: ç›®æ ‡å˜é‡åˆ—å
            
        Returns:
            æ‰€æœ‰æ¨¡å‹çš„è®­ç»ƒç»“æœ
        """
        
        print("ğŸš€ Training all available models...")
        
        # å‡†å¤‡æ•°æ®
        X, y, feature_info = self.prepare_data(df, target_column)
        
        # è®­ç»ƒæ¯ä¸ªæ¨¡å‹
        results = {}
        for model_name in self.models.keys():
            try:
                # ä½¿ç”¨æ–°çš„è¶…å‚æ•°ä¼˜åŒ–è®­ç»ƒæ–¹æ³•
                result = self.train_model_with_hyperopt(model_name, X, y)
                results[model_name] = result
                print(f"âœ… {model_name} training completed")
            except Exception as e:
                print(f"âŒ {model_name} training failed: {e}")
                continue
        
        # åˆ›å»ºé›†æˆæ¨¡å‹
        if len(results) > 1:
            try:
                ensemble_result = self._create_ensemble_model(X, y, results)
                results['ensemble'] = ensemble_result
                print("âœ… Ensemble model created")
            except Exception as e:
                print(f"âŒ Ensemble model creation failed: {e}")
        
        self.evaluation_results = results
        return results
    
    def _create_ensemble_model(self, X, y, individual_results):
        """åˆ›å»ºé›†æˆæ¨¡å‹"""
        
        print("ğŸ”— Creating ensemble model...")
        
        # å‡†å¤‡åŸºå­¦ä¹ å™¨
        estimators = []
        for name, result in individual_results.items():
            if 'model' in result:
                estimators.append((name, result['model']))
        
        # åˆ›å»ºæŠ•ç¥¨å›å½’å™¨
        ensemble = VotingRegressor(estimators=estimators)
        
        # åˆ†å‰²æ•°æ®è¿›è¡Œè¯„ä¼°
        X_train, X_test, y_train, y_test = train_test_split(
            X, y, test_size=config.TEST_SIZE, random_state=42
        )
        
        # è®­ç»ƒé›†æˆæ¨¡å‹
        ensemble.fit(X_train, y_train)
        
        # é¢„æµ‹å’Œè¯„ä¼°
        y_pred_train = ensemble.predict(X_train)
        y_pred_test = ensemble.predict(X_test)
        
        train_metrics = diagnose_model_performance(y_train, y_pred_train, "ensemble_train")
        test_metrics = diagnose_model_performance(y_test, y_pred_test, "ensemble_test")
        
        # äº¤å‰éªŒè¯
        cv_scores = cross_val_score(ensemble, X_train, y_train, 
                                   cv=config.CV_FOLDS, scoring='r2')
        
        result = {
            'model': ensemble,
            'train_metrics': train_metrics,
            'test_metrics': test_metrics,
            'cv_scores': cv_scores,
            'cv_mean': cv_scores.mean(),
            'cv_std': cv_scores.std(),
            'feature_info': self.feature_info
        }
        
        return result
    
    def compare_models(self) -> pd.DataFrame:
        """æ¯”è¾ƒæ‰€æœ‰è®­ç»ƒçš„æ¨¡å‹"""
        
        if not self.evaluation_results:
            print("âŒ No trained models to compare")
            return pd.DataFrame()
        
        comparison_data = []
        
        for model_name, result in self.evaluation_results.items():
            test_metrics = result['test_metrics']
            
            comparison_data.append({
                'Model': model_name,
                'RÂ²': test_metrics['r2'],
                'RMSE': test_metrics['rmse'],
                'MAE': test_metrics['mae'],
                'CV_Mean': result['cv_mean'],
                'CV_Std': result['cv_std']
            })
        
        comparison_df = pd.DataFrame(comparison_data)
        comparison_df = comparison_df.sort_values('RÂ²', ascending=False)
        
        print("ğŸ“Š Model Comparison:")
        print("=" * 80)
        print(comparison_df.to_string(index=False, float_format='%.4f'))
        print("=" * 80)
        
        return comparison_df
    
    def get_best_model(self) -> Tuple[str, Any]:
        """è·å–æœ€ä½³æ¨¡å‹"""
        
        if not self.evaluation_results:
            return None, None
        
        best_model_name = None
        best_score = -np.inf
        
        for model_name, result in self.evaluation_results.items():
            score = result['test_metrics']['r2']
            if score > best_score:
                best_score = score
                best_model_name = model_name
        
        return best_model_name, self.evaluation_results[best_model_name]
    
    def save_models(self, output_dir: str = None):
        """ä¿å­˜æ‰€æœ‰è®­ç»ƒçš„æ¨¡å‹"""
        
        if output_dir is None:
            output_dir = config.MODELS_DIR
        
        output_path = Path(output_dir)
        output_path.mkdir(exist_ok=True)
        
        saved_count = 0
        
        for model_name, result in self.evaluation_results.items():
            try:
                model_file = output_path / f"{model_name}_model.pkl"
                
                # ä¿å­˜æ¨¡å‹å’Œç›¸å…³ä¿¡æ¯
                save_data = {
                    'model': result['model'],
                    'feature_info': result['feature_info'],
                    'metrics': result['test_metrics'],
                    'cv_scores': result['cv_scores']
                }
                
                if save_model_safely(save_data, str(model_file)):
                    saved_count += 1
                    
            except Exception as e:
                print(f"âŒ Failed to save {model_name}: {e}")
        
        print(f"âœ… Saved {saved_count}/{len(self.evaluation_results)} models to {output_path}")

# ä¾¿æ·å‡½æ•°
def train_frp_models(df: pd.DataFrame, target_column: str = None, 
                     enable_hyperparameter_tuning: bool = True) -> Dict[str, Any]:
    """ä¾¿æ·çš„æ¨¡å‹è®­ç»ƒå‡½æ•°"""
    
    trainer = ModelTrainer(enable_hyperparameter_tuning=enable_hyperparameter_tuning)
    results = trainer.train_all_models(df, target_column)
    
    # æ˜¾ç¤ºæ¯”è¾ƒç»“æœ
    trainer.compare_models()
    
    # è·å–æœ€ä½³æ¨¡å‹
    best_name, best_result = trainer.get_best_model()
    if best_name:
        print(f"ğŸ† Best model: {best_name} (RÂ² = {best_result['test_metrics']['r2']:.4f})")
    
    return results